{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": False,
    "scrolled": False
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import sys\n",
    "import os\n",
    "from test_helper import Test\n",
    "from pyspark.sql import SQLContext\n",
    "from jdatetime import datetime as jal_datetime\n",
    "from time import mktime\n",
    "\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "baseDir = os.path.join('BigData_WorkGroup')\n",
    "inputPath = os.path.join('data')\n",
    "\n",
    "# white_list = u'ءآأؤإئابةتثجحخدذرزسشصضطظعغفقكلمنهوىي٠١٢٣٤٥٦٧٨٩پڃڄچڇڑژڤکكگھی۰۱۲۳۴۵۶۷۸۹'\n",
    "\n",
    "# GOOGLE_PATH = 'Google.csv'\n",
    "# GOOGLE_SMALL_PATH = 'Google_small.csv'\n",
    "# AMAZON_PATH = 'Amazon.csv'\n",
    "# AMAZON_SMALL_PATH = 'Amazon_small.csv'\n",
    "# GOLD_STANDARD_PATH = 'Amazon_Google_perfectMapping.csv'\n",
    "FARS_NEWS_PATH = 'FarsNews_Small.json'\n",
    "# STOPWORDS_PATH = 'stopwords.txt'\n",
    "\n",
    "def get_timestamp(date, time):\n",
    "    time = '00:00'\n",
    "    try:\n",
    "        y, m, d = map(int, date.split('/'))\n",
    "    except:\n",
    "        y, m, d = (90, 1, 1)\n",
    "    y += 1300\n",
    "    try:\n",
    "        h, mins = map(int, time.split(':'))\n",
    "    except:\n",
    "        h, mins = (0, 0)\n",
    "    date = jal_datetime(y, m, d, h, mins)\n",
    "    return int(mktime(date.timetuple()))\n",
    "\n",
    "def load_data(file_name):\n",
    "    def get_val(val, default_value = ''):\n",
    "        if val is None:\n",
    "            return default_value\n",
    "        return val[0]\n",
    "\n",
    "    file_addr = os.path.join(inputPath, file_name)\n",
    "    json_file = sqlContext.jsonFile(file_addr)\n",
    "    data = json_file.flatMap(lambda row: row.data)\n",
    "    print data.count()\n",
    "#         formatted_data = data.map(lambda rec: (rec.pre_title[0], rec.title, rec.abstract))\n",
    "    formatted_data = data.map(lambda rec: (rec._pageUrl, (get_val(rec.pre_title), get_val(rec.title),\n",
    "                                                          get_val(rec.abstract), get_val(rec.body),\n",
    "                                                          rec.category, [],  # couldn't find tags in json file\n",
    "                                                          get_timestamp(get_val(rec.date, '90/01/01'),\n",
    "                                                                        get_val(rec.time, '00:00')),\n",
    "                                                          0,  # also didn't find visit_count\n",
    "                                                          0  # for comments, i need an example with an actual comment\n",
    "                                                             #   rec.comment has some details but not good enough\n",
    "                                                          )\n",
    "                                          )\n",
    "                             )\n",
    "    return formatted_data.cache()\n",
    "\n",
    "ch_list = {u'ئ': u'ى', u'ي': u'ى', u'ك': u'ک'}\n",
    "# def arabic_to_farsi(c):\n",
    "#     if c in ch_list:\n",
    "#         return ch_list[c]\n",
    "#     else:\n",
    "#         return c\n",
    "\n",
    "def tokenize(s):\n",
    "    clean_s = re.sub(u'[^\\u200cءآأؤإئابةتثجحخدذرزسشصضطظعغفقكلمنهوىي٠١٢٣٤٥٦٧٨٩پڃڄچڇڑژڤکكگھی۰۱۲۳۴۵۶۷۸۹\\s0-9A-Za-z]', '', s)\n",
    "#     clean_s = ''.join([arabic_to_farsi(c) for c in clean_s])\n",
    "#     for ch in ch_list:\n",
    "#         clean_s = re.sub(ch, ch_list[ch], clean_s)\n",
    "    return clean_s.split()\n",
    "\n",
    "def tf(tokens):\n",
    "    \"\"\" Compute TF\n",
    "    Args:\n",
    "        tokens (list of str): input list of tokens from tokenize\n",
    "    Returns:\n",
    "        dictionary: a dictionary of tokens to its TF values\n",
    "    \"\"\"\n",
    "    out = dict()\n",
    "    for token in tokens:\n",
    "        if token not in out:\n",
    "            out[token] = 1. / len(tokens)\n",
    "        else:\n",
    "            out[token] += 1. / len(tokens)\n",
    "    return out\n",
    "\n",
    "def norm(a):\n",
    "    \"\"\" Compute square root of the dot product\n",
    "    Args:\n",
    "        a (dictionary): a dictionary of record to value\n",
    "    Returns:\n",
    "        norm: a dictionary of tokens to its TF values\n",
    "    \"\"\"\n",
    "    return sum([a[token] ** 2 for token in a]) ** 0.5\n",
    "\n",
    "\n",
    "\n",
    "def idfs(corpus):\n",
    "    \"\"\" Compute IDF\n",
    "    Args:\n",
    "        corpus (RDD): input corpus\n",
    "    Returns:\n",
    "        RDD: a RDD of (token, IDF value)\n",
    "    \"\"\"\n",
    "    N = corpus.count()\n",
    "    uniqueTokens = corpus.flatMap(lambda rec: list(set(rec[1])))\n",
    "    tokenCountPairTuple = uniqueTokens.map(lambda s: (s, 1))\n",
    "    tokenSumPairTuple = tokenCountPairTuple.reduceByKey(lambda x, y: x+y)\n",
    "    return (tokenSumPairTuple.map(lambda rec: (rec[0], float(N) / rec[1])))\n",
    "\n",
    "\n",
    "def tfidf(tokens, idfs):\n",
    "    \"\"\" Compute TF-IDF\n",
    "    Args:\n",
    "        tokens (list of str): input list of tokens from tokenize\n",
    "        idfs (dictionary): record to IDF value\n",
    "    Returns:\n",
    "        dictionary: a dictionary of records to TF-IDF values\n",
    "    \"\"\"\n",
    "    tfs = tf(tokens)\n",
    "    tfIdfDict = {token: tfs[token] * idfs[token] for token in tokens}\n",
    "    return tfIdfDict\n",
    "\n",
    "def invert(record):\n",
    "        \"\"\" Invert (ID, tokens) to a list of (token, ID)\n",
    "        Args:\n",
    "            record: a pair, (ID, token vector)\n",
    "        Returns:\n",
    "            pairs: a list of pairs of token to ID\n",
    "        \"\"\"\n",
    "        return ([(token, record[0]) for token in record[1]])\n",
    "\n",
    "    \n",
    "def swap(record):\n",
    "    \"\"\" Swap (token, (ID, URL)) to ((ID, URL), token)\n",
    "    Args:\n",
    "        record: a pair, (token, (ID, URL))\n",
    "    Returns:\n",
    "        pair: ((ID, URL), token)\n",
    "    \"\"\"\n",
    "    token = record[0]\n",
    "    keys = record[1]\n",
    "    return (keys, token)\n",
    "\n",
    "\n",
    "def find_similarities(news1, news2):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        news -> RDD of (id, text)\n",
    "    Returns:\n",
    "        RDD list of [((id1, id2), similiarity_measure)]\n",
    "    \"\"\"\n",
    "    news1_tokens = news1.map(lambda rec: (rec[0], tokenize(rec[1]))).cache()\n",
    "    news2_tokens = news2.map(lambda rec: (rec[0], tokenize(rec[1]))).cache()\n",
    "    fullCorpus = news1_tokens.union(news2_tokens)\n",
    "    idfsFull = idfs(fullCorpus)\n",
    "    idfsFullWeights = idfsFull.collectAsMap()\n",
    "    idfsFullBroadcast = sc.broadcast(idfsFullWeights)\n",
    "    news1_weights = news1_tokens.map(lambda rec: (rec[0], tfidf(rec[1], idfsFullBroadcast.value)))\n",
    "    news2_weights = news2_tokens.map(lambda rec: (rec[0], tfidf(rec[1], idfsFullBroadcast.value)))\n",
    "    news1_norms = news1_weights.map(lambda rec: (rec[0], norm(rec[1]))).collectAsMap()\n",
    "    news1_norms_broadcast = sc.broadcast(news1_norms)\n",
    "    news2_norms = news2_weights.map(lambda rec: (rec[0], norm(rec[1]))).collectAsMap()\n",
    "    news2_norms_broadcast = sc.broadcast(news2_norms)\n",
    "\n",
    "    news1_inv_pairs = (news1_weights\n",
    "                        .flatMap(invert)\n",
    "                        .cache())\n",
    "\n",
    "    news2_inv_pairs = (news2_weights\n",
    "                        .flatMap(invert)\n",
    "                        .cache())\n",
    "    \n",
    "    commonTokens = (news1_inv_pairs\n",
    "                .join(news2_inv_pairs)\n",
    "                .map(swap)\n",
    "                .groupByKey()\n",
    "                .cache())\n",
    "    \n",
    "    news1_weights_broadcast = sc.broadcast(news1_weights.collectAsMap())\n",
    "    news2_weights_broadcast = sc.broadcast(news2_weights.collectAsMap())\n",
    "    \n",
    "    def fastCosineSimilarity(record):\n",
    "        \"\"\" Compute Cosine Similarity using Broadcast variables\n",
    "        Args:\n",
    "            record: ((ID, URL), token)\n",
    "        Returns:\n",
    "            pair: ((ID, URL), cosine similarity value)\n",
    "        \"\"\"\n",
    "        news1_rec = record[0][0]\n",
    "        news2_rec = record[0][1]\n",
    "        tokens = record[1]\n",
    "        s = sum([news1_weights_broadcast.value[news1_rec][tok] * news2_weights_broadcast.value[news2_rec][tok] \n",
    "                    for tok in tokens])\n",
    "        value = float(s) / news2_norms_broadcast.value[news2_rec] / news1_norms_broadcast.value[news1_rec]\n",
    "        key = (news1_rec, news2_rec)\n",
    "        return (key, value)\n",
    "    \n",
    "    similaritiesFullRDD = (commonTokens\n",
    "                       .map(fastCosineSimilarity)\n",
    "                       .cache())\n",
    "    \n",
    "    print similaritiesFullRDD.filter(lambda rec: rec[1] > 0.4).take(2)\n",
    "    return similaritiesFullRDD\n",
    "    \n",
    "# farsnews_small = load_data(FARS_NEWS_PATH)\n",
    "\n",
    "def subtext_tokenizer(tup):\n",
    "    \"\"\"\n",
    "    input: news_tuple -> (url, (pre_title, title, abstract, \n",
    "                                body, categories, tags, \n",
    "                                timestamp, visit_count, comment_count))\n",
    "    output: ((timestamp, keyword), 1)\n",
    "    \"\"\"\n",
    "    token_list = tokenize(tup[1][0] + ' ' + tup[1][1] + ' ' + tup[1][2] + ' ' + tup[1][3])\n",
    "    return [((tup[1][6], kw), 1) for kw in token_list]\n",
    "\n",
    "# testRDD = sc.parallelize([('http://salam', ('گفت', 'شروع مدارس', 'اول مهرماه', 'زنگ مدرسه به صدا درآمد', [], [], 735862, 10, 10)),\n",
    "# ('http://salam2', ('گفت', 'شروع مدارس، روز اول كاری', 'اول مهرماه', 'شروع دوباره ي مدارس تهران', [], [], 735861, 10, 10))])\n",
    "# tokenCount = testRDD.flatMap(subtext_tokenizer).reduceByKey(lambda x, y: x+y)\n",
    "# for c in tokenCount.collect():\n",
    "#     print c[0][0], c[0][1], c[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": False
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2370\n"
     ]
    }
   ],
   "source": [
    "farsnews_small = load_data(FARS_NEWS_PATH)\n",
    "# print farsnews_small.take(2)\n",
    "tokenRDD = farsnews_small.flatMap(subtext_tokenizer).reduceByKey(lambda x, y: x+y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": False
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 انسان\n",
      "3 از\n",
      "3 اىمه\n",
      "3 کتاب\n",
      "3 250ساله\n",
      "2 سىاسى\n",
      "2 سىره\n",
      "2 با\n",
      "2 درباره\n",
      "2 است\n",
      "2 که\n",
      "2 معظم\n",
      "2 اىن\n",
      "1 دىدگاه‌هاى\n",
      "1 پىامبر\n",
      "1 سال\n",
      "1 کشىده\n",
      "1 متفاوت\n",
      "1 دارند\n",
      "1 اىشان\n"
     ]
    }
   ],
   "source": [
    "for c in (tokenRDD.filter(lambda rec: int(rec[0][0]) == 1312071720)\n",
    "                   .map(lambda rec: (rec[1], rec[0][1]))\n",
    "                   .takeOrdered(20, lambda rec: -rec[0])):\n",
    "    print c[0], c[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": False
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "468"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "farsnews_small.count()\n",
    "farsnews_small_trans = (farsnews_small\n",
    "                        .filter(lambda rec: random.random() < 0.2)  # it took too much memory and space -\n",
    "                                                                    #   for me to do the whole set.\n",
    "                        .map(lambda rec: (rec[0], rec[1][3]))\n",
    "                        .cache())\n",
    "farsnews_small_trans.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": False
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[((u'http://www.farsnews.com/newstext.php?nn=13931206000797', u'http://www.farsnews.com/newstext.php?nn=13931206000797'), 1.0000000000000002), ((u'http://www.farsnews.com/newstext.php?nn=13930424000679', u'http://www.farsnews.com/newstext.php?nn=13930424000679'), 1.0000000000000002)]\n"
     ]
    }
   ],
   "source": [
    "out = find_similarities(farsnews_small_trans, farsnews_small_trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": False
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n",
      "[((u'http://www.farsnews.com/newstext.php?nn=13940230001168', u'http://www.farsnews.com/newstext.php?nn=13940209001269'), 0.7996507062112804), ((u'http://www.farsnews.com/newstext.php?nn=13931125000586', u'http://www.farsnews.com/newstext.php?nn=13940508000923'), 0.793197024916902), ((u'http://www.farsnews.com/newstext.php?nn=13930410000332', u'http://www.farsnews.com/newstext.php?nn=13940403000348'), 0.5326030154877435), ((u'http://www.farsnews.com/newstext.php?nn=13940406001171', u'http://www.farsnews.com/newstext.php?nn=13940327001032'), 0.6454314520424052), ((u'http://www.farsnews.com/newstext.php?nn=13940209001269', u'http://www.farsnews.com/newstext.php?nn=13940230001168'), 0.7996507062112805), ((u'http://www.farsnews.com/newstext.php?nn=13940403000348', u'http://www.farsnews.com/newstext.php?nn=13930410000332'), 0.5326030154877435), ((u'http://www.farsnews.com/newstext.php?nn=13940508000923', u'http://www.farsnews.com/newstext.php?nn=13931120000127'), 0.793197024916902), ((u'http://www.farsnews.com/newstext.php?nn=13940327001032', u'http://www.farsnews.com/newstext.php?nn=13940406001171'), 0.6454314520424053), ((u'http://www.farsnews.com/newstext.php?nn=13940401000225', u'http://www.farsnews.com/newstext.php?nn=13930410000332'), 0.5326030154877435), ((u'http://www.farsnews.com/newstext.php?nn=13931120000127', u'http://www.farsnews.com/newstext.php?nn=13940508000923'), 0.793197024916902), ((u'http://www.farsnews.com/newstext.php?nn=13930410000332', u'http://www.farsnews.com/newstext.php?nn=13940401000225'), 0.5326030154877435), ((u'http://www.farsnews.com/newstext.php?nn=13940508000923', u'http://www.farsnews.com/newstext.php?nn=13931125000586'), 0.793197024916902), ((u'http://www.farsnews.com/newstext.php?nn=13940407000303', u'http://www.farsnews.com/newstext.php?nn=13930410000332'), 0.5326030154877435), ((u'http://www.farsnews.com/newstext.php?nn=13940327001032', u'http://www.farsnews.com/newstext.php?nn=13940421001171'), 0.7224546094582998), ((u'http://www.farsnews.com/newstext.php?nn=13940421001171', u'http://www.farsnews.com/newstext.php?nn=13940406001171'), 0.5604987958666678), ((u'http://www.farsnews.com/newstext.php?nn=13940325000735', u'http://www.farsnews.com/newstext.php?nn=13940512000226'), 0.7241488345100918), ((u'http://www.farsnews.com/newstext.php?nn=13930410000332', u'http://www.farsnews.com/newstext.php?nn=13940407000303'), 0.5326030154877435), ((u'http://www.farsnews.com/newstext.php?nn=13940512000226', u'http://www.farsnews.com/newstext.php?nn=13940325000735'), 0.7241488345100918), ((u'http://www.farsnews.com/newstext.php?nn=13940406001171', u'http://www.farsnews.com/newstext.php?nn=13940421001171'), 0.5604987958666678), ((u'http://www.farsnews.com/newstext.php?nn=13940421001171', u'http://www.farsnews.com/newstext.php?nn=13940327001032'), 0.7224546094582996)]\n"
     ]
    }
   ],
   "source": [
    "goods = out.filter(lambda rec: rec[1] < 0.9 and rec[1] > 0.5)  # more than 1 means the same inputs i think\n",
    "print goods.count()\n",
    "print goods.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": False
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://www.farsnews.com/newstext.php?nn=13940325000735 http://www.farsnews.com/newstext.php?nn=13940512000226\n",
      "به گزارش خبرنگار آیین و اندیشه خبرگزاری فارس، نماز بهترین اعمال دینی است که اگر قبول درگاه خداوند عالم شود، عبادت‌های دیگر هم قبول می‌شود و اگر پذیرفته نشود اعمال دیگر هم قبول نمی‌شود، همان‌طور که اگر انسان شبانه‌روزی پنج نوبت در نهر آبی شست‌وشو کند، آلودگی در بدنش نمی‌ماند، نمازهای پنج‌گانه هم انسان را از گناهان پاک می‌کند و سزاوار است که انسان نماز را در اوّل وقت بخواند و کسی که نماز را پست و سبک شمارد مانند کسی است که نماز نمی‌خواند، با این وجود یکی از سؤالاتی که مطرح می‌شود این است که چرا کودکان و نوجوانان از نماز رویگردان هستند، حجت‌الاسلام والمسلمین محسن قرائتی رئیس ستاد اقامه نماز در پاسخ به این پرسش در کتاب شیوه‌های دعوت به نماز می‌نویسد:\n",
      "به گزارش خبرنگار آیین و اندیشه خبرگزاری فارس، نماز بهترین اعمال دینی است که اگر قبول درگاه خداوند عالم شود، عبادت‌های دیگر هم قبول می‌شود و اگر پذیرفته نشود اعمال دیگر هم قبول نمی‌شود، همان ‌طور که اگر انسان شبانه‌روزی پنج نوبت در نهر آبی شست‌وشو کند، آلودگی در بدنش نمی‌ماند، نمازهای پنج‌گانه هم انسان را از گناهان پاک می‌کند و سزاوار است که انسان نماز را در اوّل وقت بخواند و کسی که نماز را پست و سبک شمارد مانند کسی است که نماز نمی‌خواند.\n"
     ]
    }
   ],
   "source": [
    "n1, n2 = goods.collect()[15][0]\n",
    "print n1, n2\n",
    "# print farsnews_small.filter(lambda rec: rec[0] == n1).collect()\n",
    "print farsnews_small.filter(lambda rec: rec[0] == n1).take(1)[0][1][3]\n",
    "print farsnews_small.filter(lambda rec: rec[0] == n2).take(1)[0][1][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": None,
   "metadata": {
    "collapsed": True
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
